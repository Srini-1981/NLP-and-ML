{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Purpose of this algorithm\n",
    "* Build an algorithm to answer the questions from document which has question and answer markers.\n",
    "* Note: This algorithm works only if the document has markers.\n",
    "\n",
    "#### High level steps of the algorithm\n",
    "* Read the documents from the folder which has the markers, below are the markers used for question and answer extraction.\n",
    "    * question_start = 'QUE-S'\n",
    "    * question_end = 'QUE-E'\n",
    "    * answer_start = 'ANS-S'\n",
    "    * answer_end = 'ANS-E'\n",
    "* Create the question and answering knowledge base.\n",
    "* Create a search index to understand the question and return the answer.\n",
    "* Used soft cosine similarity from gensim in this algorithm for search index, this is to bring in semantic meaning to the sentence or question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packages \n",
    "import os\n",
    "\n",
    "import gensim.downloader as api\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import Word2Vec, WordEmbeddingSimilarityIndex\n",
    "from gensim.similarities import SoftCosineSimilarity, SparseTermSimilarityMatrix\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "from docx import Document\n",
    "import pandas as pd\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Read the document, loop through all the paragraphs and return them as a text\n",
    "\"\"\"\n",
    "def create_doc_text(doc):\n",
    "    doc_paragraphs = []\n",
    "    for para in doc.paragraphs:\n",
    "        doc_paragraphs.append(para.text)\n",
    "    \n",
    "    doc_paragraphs = list(filter(None, doc_paragraphs))\n",
    "    \n",
    "    doc_text = ''\n",
    "    for doc_paragraph in doc_paragraphs:\n",
    "        doc_text = doc_text + doc_paragraph\n",
    "    \n",
    "    return doc_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Split the sentences with required markers from upcoming functions\n",
    "\"\"\"\n",
    "def split(text, start, end):\n",
    "    s_splits = text.split(start)\n",
    "    s_splits = list(filter(None, s_splits))\n",
    "    \n",
    "    e_splits = []\n",
    "    for split in s_splits:\n",
    "        e_splits.append(split.split(end))\n",
    "    \n",
    "    return e_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create knowledge base function by integrating above two functions\n",
    "\"\"\"\n",
    "def create_knowledge(doc):\n",
    "    # create the markers for question and answer pair extraction\n",
    "    q_start = 'QUE-S'\n",
    "    q_end = 'QUE-E'\n",
    "    a_start = 'ANS-S'\n",
    "    a_end = 'ANS-E'\n",
    "    \n",
    "    doc_text = create_doc_text(doc)\n",
    "    q_splits = split(doc_text.strip(), q_start, q_end)\n",
    "    \n",
    "    q_a_pairs = []\n",
    "    for i in range(len(q_splits)):\n",
    "        question = q_splits[i][0]\n",
    "        answer = q_splits[i][1]\n",
    "        answer = split(answer.strip(), a_start, a_end)\n",
    "        #print(answer)\n",
    "        answer = answer[0][0]\n",
    "        q_a_pairs.append((question.strip(), answer.strip()))\n",
    "    \n",
    "    return q_a_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create the knowledgebase from the document \n",
    "\"\"\"\n",
    "data_dir = \"C:\\\\srini\\\\qna\\\\word\"\n",
    "file_name = \"XXX_HR_XXX_Code_FAQ.docx\"\n",
    "data_path = os.path.join(data_dir, file_name)\n",
    "\n",
    "doc = Document(data_path)\n",
    "q_a_pairs = create_knowledge(doc)\n",
    "knowledge_base = pd.DataFrame(q_a_pairs, columns =['question', 'answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our team is not in a client-facing or sales ro...</td>\n",
       "      <td>Whether your role requires you to interact wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why are the physical security teams instructed...</td>\n",
       "      <td>A people manager could have a number of member...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Does this policy apply to all members, includi...</td>\n",
       "      <td>The dress code policy is applicable to all mem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Formal attire during high-profile business mee...</td>\n",
       "      <td>CGI believes that members should maintain a pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Is Jeans paired with a formal shirt considered...</td>\n",
       "      <td>No, denim clothing or footwear is not acceptab...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  Our team is not in a client-facing or sales ro...   \n",
       "1  Why are the physical security teams instructed...   \n",
       "2  Does this policy apply to all members, includi...   \n",
       "3  Formal attire during high-profile business mee...   \n",
       "4  Is Jeans paired with a formal shirt considered...   \n",
       "\n",
       "                                              answer  \n",
       "0  Whether your role requires you to interact wit...  \n",
       "1  A people manager could have a number of member...  \n",
       "2  The dress code policy is applicable to all mem...  \n",
       "3  CGI believes that members should maintain a pr...  \n",
       "4  No, denim clothing or footwear is not acceptab...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check the created knowledgebase\n",
    "knowledge_base.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get stop words from nltk\n",
    "stopWords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['jeans', 'paired', 'formal', 'shirt', 'considered', 'business', 'casuals'],\n",
       " ['work', 'night', 'shift.', 'need', 'follow', 'dress', 'code']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pre processing data function\n",
    "def clean_data(sentence):\n",
    "    # convert to lowercase, ignore all special characters - \n",
    "    # keep only alpha-numericals and spaces (not removing full-stop here)\n",
    "    sentence = re.sub(r'[^A-Za-z0-9\\s.]', r'', str(sentence).lower())\n",
    "    sentence = re.sub(r'\\n', r' ', sentence)\n",
    "    \n",
    "    # remove stop words\n",
    "    sentence = \" \".join([word for word in sentence.split() if word not in stopWords])\n",
    "    \n",
    "    return sentence.split()\n",
    "\n",
    "# Pre-process all the questions\n",
    "questions_list = knowledge_base.question.map(lambda x: clean_data(x))\n",
    "# Make list of lists to feed the data into the algorithm\n",
    "questions_list = questions_list.tolist()\n",
    "# Sanity check the documents after pre-processing\n",
    "questions_list[4:6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-25 20:07:26,914 : INFO : loading projection weights from C:\\Users\\mommasani.srinivasul/gensim-data\\word2vec-google-news-300\\word2vec-google-news-300.gz\n",
      "2020-02-25 20:10:24,073 : INFO : loaded (3000000, 300) matrix from C:\\Users\\mommasani.srinivasul/gensim-data\\word2vec-google-news-300\\word2vec-google-news-300.gz\n"
     ]
    }
   ],
   "source": [
    "# methods to train word-vectors\n",
    "\"\"\"\n",
    "Using below we can build our own word to vector embeddings \n",
    "\"\"\"\n",
    "#w2v_model = Word2Vec(questions_list, size=50, min_count=1, iter=50)  \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Using below, we are loading pre-trained word vectors from the large corpus\n",
    "List of pre-trained word vectors available in gensim, below link for details\n",
    "https://github.com/RaRe-Technologies/gensim-data\n",
    "\"\"\"\n",
    "#w2v_model = api.load(\"glove-wiki-gigaword-50\")\n",
    "w2v_model = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-25 20:10:24,116 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6258443\n",
      "[('Rourkela', 0.7121847867965698), ('Asansol', 0.7043161392211914), ('Dankuni', 0.6961910724639893), ('Bokaro', 0.6898972392082214), ('Howrah', 0.6849250793457031), ('Sambalpur', 0.6841989159584045), ('Jhargram', 0.683922290802002), ('Burdwan', 0.6817904114723206), ('Uluberia', 0.6772913336753845), ('Bilaspur', 0.6764135360717773)]\n",
      "300\n",
      "3000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mommasani.srinivasul\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\mommasani.srinivasul\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# Sanity checks for the word2vec model\n",
    "print(w2v_model.similarity('maharaja','maharani'))\n",
    "print(w2v_model.most_similar('Kharagpur'))\n",
    "print(len(w2v_model.wv['save']))\n",
    "print(len(w2v_model.wv.vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mommasani.srinivasul\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n",
      "2020-02-25 20:10:33,541 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-02-25 20:10:33,544 : INFO : built Dictionary(73 unique tokens: ['clientfacing', 'code', 'dress', 'necessary', 'role']...) from 11 documents (total 101 corpus positions)\n",
      "2020-02-25 20:10:33,546 : INFO : constructing a sparse term similarity matrix using <gensim.models.keyedvectors.WordEmbeddingSimilarityIndex object at 0x00000233D1FC48C8>\n",
      "2020-02-25 20:10:33,548 : INFO : iterating over columns in dictionary order\n",
      "2020-02-25 20:10:33,553 : INFO : PROGRESS: at 1.37% columns (1 / 73, 1.369863% density, 1.369863% projected density)\n",
      "2020-02-25 20:10:51,776 : INFO : constructed a sparse term similarity matrix with 2.720961% density\n"
     ]
    }
   ],
   "source": [
    "# Construct term simiarity index the trained or pre-trained word 2 vectors\n",
    "termsim_index = WordEmbeddingSimilarityIndex(w2v_model.wv)\n",
    "# Construct the dictionary from our documents\n",
    "dictionary = Dictionary(questions_list)\n",
    "# Construct document to bag of words using the dictionary built above\n",
    "bow_corpus = [dictionary.doc2bow(question) for question in questions_list]\n",
    "# Construct similarity matrix using term similarity index built from word2vec and from the \n",
    "# document dictionary\n",
    "similarity_matrix = SparseTermSimilarityMatrix(termsim_index, dictionary)  \n",
    "# Construct final document similarity index\n",
    "scs_sim_index = SoftCosineSimilarity(bow_corpus, similarity_matrix, num_best=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confidence score --> 1.0 for record --> Is Jeans paired with a formal shirt considered business casuals?\n",
      "confidence score --> 0.5443339347839355 for record --> Formal attire during high-profile business meetings and visits is fine but why do we need to have a dress code throughout the week?\n",
      "confidence score --> 0.23741625249385834 for record --> Due to medical reasons, I need to wear certain clothing or footwear. Can I be exempted from the dress code?\n",
      "confidence score --> 0.17929314076900482 for record --> Our team is not in a client-facing or sales role, why is a dress code necessary?\n",
      "confidence score --> 0.16627000272274017 for record --> The dress code is very restrictive. I cycle to work every day and so wear biking attire, I later change at work. Will the security team allow me to enter the facility?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mommasani.srinivasul\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\similarities\\termsim.py:358: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  Y = np.multiply(Y, 1 / np.sqrt(Y_norm))\n",
      "C:\\Users\\mommasani.srinivasul\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\similarities\\termsim.py:358: RuntimeWarning: invalid value encountered in multiply\n",
      "  Y = np.multiply(Y, 1 / np.sqrt(Y_norm))\n"
     ]
    }
   ],
   "source": [
    "# make a query\n",
    "query = 'jeans and formal shirt are business casual?'\n",
    "query = clean_data(query)\n",
    "\n",
    "# calculate similarity of query to each doc from bow_corpus\n",
    "scs_sims = scs_sim_index[dictionary.doc2bow(query)]\n",
    "for i in range(len(scs_sims)):\n",
    "    print(\"confidence score --> {} for record --> {}\" .format(scs_sims[i][1], knowledge_base.question[scs_sims[i][0]]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
